---
phase: 09-chunked-document-review
plan: 03
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - backend/app/ai/reviewer.py
autonomous: true

must_haves:
  truths:
    - "Documents over 40 pages are automatically chunked"
    - "Chunks are processed in parallel with concurrency limit of 2"
    - "Issues from overlapping regions are deduplicated"
    - "Partial failures return completed chunks with error for failed chunks"
  artifacts:
    - path: "backend/app/ai/reviewer.py"
      provides: "Chunked review orchestration with parallel processing"
      exports: ["review_document", "review_document_chunked"]
  key_links:
    - from: "backend/app/ai/reviewer.py"
      to: "backend/app/ai/chunker.py"
      via: "chunker import"
      pattern: "from \\.chunker import"
    - from: "backend/app/ai/reviewer.py"
      to: "asyncio.Semaphore"
      via: "concurrency control"
      pattern: "Semaphore\\("
---

<objective>
Implement chunked review orchestration with parallel processing and deduplication.

Purpose: Enable reviewing large documents by processing chunks in parallel while limiting API calls
Output: Modified reviewer.py with automatic chunking detection, parallel chunk processing, and issue deduplication
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-chunked-document-review/09-RESEARCH.md
@.planning/phases/09-chunked-document-review/09-01-SUMMARY.md
@backend/app/ai/reviewer.py
@backend/app/ai/client.py
@backend/app/ai/chunker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add issue deduplication function</name>
  <files>backend/app/ai/reviewer.py</files>
  <action>
Add deduplication function to reviewer.py:

```python
import hashlib
from typing import List, Dict, Any

def deduplicate_issues(all_issues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove duplicate issues from chunk overlaps.

    Deduplicates by hashing (title + min_page + category).
    For duplicates, keeps the first occurrence.

    Args:
        all_issues: List of issue dicts from parsed JSON

    Returns:
        Deduplicated list of issues
    """
    seen = set()
    unique = []

    for issue in all_issues:
        # Create hash key from issue identity
        pages = issue.get('pages', [1])
        min_page = min(pages) if pages else 1
        key_parts = f"{issue.get('title', '')}|{min_page}|{issue.get('category', '')}"
        key = hashlib.md5(key_parts.encode()).hexdigest()

        if key not in seen:
            seen.add(key)
            unique.append(issue)

    return unique
```

This function will be used by the chunked review to merge and deduplicate issues from multiple chunks.
  </action>
  <verify>
Run: python -c "from app.ai.reviewer import deduplicate_issues; issues = [{'title': 'Test', 'pages': [5], 'category': 'error'}, {'title': 'Test', 'pages': [5], 'category': 'error'}, {'title': 'Other', 'pages': [10], 'category': 'warning'}]; print(len(deduplicate_issues(issues)))"
Expected: 2
  </verify>
  <done>
deduplicate_issues function removes duplicates based on title + min_page + category hash.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create review_document_chunked generator function</name>
  <files>backend/app/ai/reviewer.py</files>
  <action>
Add new async generator function for chunked review:

```python
import asyncio
import json
from .chunker import DocumentChunker, extract_page_range, filter_extraction_for_chunk
from .prompts import build_chunk_user_prompt

async def review_document_chunked(
    pdf_bytes: bytes,
    extraction: ExtractionResult,
    document_type: str,
    confidence: float,
    output_format: str = "digital",
) -> AsyncGenerator[str, None]:
    """
    Review large document in chunks with parallel processing.

    Yields:
        JSON-encoded SSE events:
        - review_start: {"total_chunks": N, "message": "..."}
        - chunk_progress: {"chunk": N, "total": M, "pages": "X-Y", "status": "processing|complete|error"}
        - text: {"text": "chunk content..."}
        - complete: {"status": "complete"}
        - error: {"error": "...", "type": "..."}
    """
    chunker = DocumentChunker()
    chunks = chunker.calculate_chunks(extraction.metadata.page_count)
    total_chunks = len(chunks)

    # DPI requirements by output format
    DPI_REQUIREMENTS = {
        "digital": {"min": 72, "label": "Digital"},
        "print": {"min": 300, "label": "Print"},
        "both": {"min": 150, "label": "Print + Digital"},
    }
    dpi_info = DPI_REQUIREMENTS.get(output_format, DPI_REQUIREMENTS["digital"])

    # Load rules context
    rules_context = load_rules_context(document_type)
    system_prompt = build_system_prompt(rules_context)

    logger.info(f"Starting chunked review: {total_chunks} chunks, {extraction.metadata.page_count} pages")

    # Emit start event
    yield json.dumps({"event": "review_start", "total_chunks": total_chunks})

    semaphore = asyncio.Semaphore(chunker.MAX_CONCURRENT)
    all_content = [""] * total_chunks  # Store content by chunk index
    all_issues = []
    errors = []

    async def process_chunk(chunk_idx: int, start: int, end: int) -> tuple:
        """Process a single chunk and return (index, content, issues, error)."""
        async with semaphore:
            try:
                # 1-indexed page numbers for prompts
                actual_start = start + 1
                actual_end = end  # end is exclusive in chunks, so this is correct for inclusive display

                logger.info(f"Processing chunk {chunk_idx + 1}/{total_chunks}: pages {actual_start}-{actual_end}")

                # Extract chunk PDF
                chunk_pdf = extract_page_range(pdf_bytes, start, end)

                # Filter extraction for this chunk
                chunk_extraction = filter_extraction_for_chunk(extraction, start, end)
                extraction_json = json.dumps(chunk_extraction.dict(), indent=2, default=str)

                # Build chunk-specific prompt
                is_first = chunk_idx == 0
                user_prompt = build_chunk_user_prompt(
                    extraction_json=extraction_json,
                    document_type=document_type,
                    confidence=confidence,
                    output_format=output_format,
                    dpi_min=dpi_info["min"],
                    chunk_start=actual_start,
                    chunk_end=actual_end,
                    is_first_chunk=is_first,
                    total_chunks=total_chunks,
                    chunk_number=chunk_idx + 1,
                )

                # Stream from AI client
                client = get_ai_client()
                content_parts = []
                async for text in client.review_document_stream(
                    pdf_bytes=chunk_pdf,
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                ):
                    content_parts.append(text)

                content = "".join(content_parts)

                # Parse issues from this chunk's content
                chunk_issues = parse_chunk_issues(content)

                return (chunk_idx, content, chunk_issues, None)

            except Exception as e:
                logger.error(f"Chunk {chunk_idx + 1} failed: {e}")
                return (chunk_idx, "", [], str(e))

    # Create tasks for all chunks
    tasks = [
        process_chunk(i, start, end)
        for i, (start, end) in enumerate(chunks)
    ]

    # Process as they complete
    completed = 0
    for coro in asyncio.as_completed(tasks):
        chunk_idx, content, chunk_issues, error = await coro
        start, end = chunks[chunk_idx]
        completed += 1

        if error:
            errors.append({"chunk": chunk_idx + 1, "pages": f"{start + 1}-{end}", "error": error})
            yield json.dumps({
                "event": "chunk_progress",
                "chunk": chunk_idx + 1,
                "total": total_chunks,
                "pages": f"{start + 1}-{end}",
                "status": "error",
                "error": error,
            })
        else:
            all_content[chunk_idx] = content
            all_issues.extend(chunk_issues)
            yield json.dumps({
                "event": "chunk_progress",
                "chunk": chunk_idx + 1,
                "total": total_chunks,
                "pages": f"{start + 1}-{end}",
                "status": "complete",
            })

    # Merge and deduplicate results
    if any(all_content):
        # Deduplicate issues from overlapping regions
        unique_issues = deduplicate_issues(all_issues)

        # Build merged content with overview noting chunked review
        merged = build_merged_review(all_content, unique_issues, errors, total_chunks)

        # Emit merged content as text event
        yield json.dumps({"event": "text", "text": merged})

    # Emit completion
    if errors:
        yield json.dumps({
            "event": "complete",
            "status": "partial",
            "completed_chunks": total_chunks - len(errors),
            "failed_chunks": len(errors),
            "errors": errors,
        })
    else:
        yield json.dumps({"event": "complete", "status": "complete"})


def parse_chunk_issues(content: str) -> List[Dict[str, Any]]:
    """Parse issues from chunk content JSON block."""
    import re
    json_match = re.search(r'```(?:json)?\s*\n(\{[\s\S]*?\})\s*\n```', content)
    if not json_match:
        return []
    try:
        parsed = json.loads(json_match.group(1))
        return parsed.get('issues', [])
    except json.JSONDecodeError:
        return []


def build_merged_review(
    chunk_contents: List[str],
    unique_issues: List[Dict[str, Any]],
    errors: List[Dict[str, Any]],
    total_chunks: int,
) -> str:
    """Build merged review from chunk contents."""
    # Collect sections from each chunk
    all_needs_attention = []
    all_looking_good = []
    all_suggestions = []

    import re
    for content in chunk_contents:
        if not content:
            continue

        # Extract sections (simplified parsing)
        na_match = re.search(r'###?\s*Needs\s*Attention\n([\s\S]*?)(?=###|```|$)', content, re.I)
        if na_match:
            text = na_match.group(1).strip()
            if text and text.lower() != "everything looks good!":
                all_needs_attention.append(text)

        lg_match = re.search(r'###?\s*Looking\s*Good\n([\s\S]*?)(?=###|```|$)', content, re.I)
        if lg_match:
            all_looking_good.append(lg_match.group(1).strip())

        sg_match = re.search(r'###?\s*Suggestions\n([\s\S]*?)(?=###|```|$)', content, re.I)
        if sg_match:
            all_suggestions.append(sg_match.group(1).strip())

    # Build merged output
    merged = "### Overview\n"
    merged += f"This document was reviewed in {total_chunks} chunks due to its length.\n"
    if errors:
        merged += f"Note: {len(errors)} chunk(s) failed to process.\n"
    merged += "\n"

    if all_needs_attention:
        merged += "### Needs Attention\n"
        merged += "\n\n".join(filter(None, all_needs_attention)) + "\n\n"
    else:
        merged += "### Needs Attention\nEverything looks good!\n\n"

    if all_looking_good:
        merged += "### Looking Good\n"
        merged += "\n\n".join(filter(None, all_looking_good)) + "\n\n"

    if all_suggestions:
        merged += "### Suggestions\n"
        merged += "\n\n".join(filter(None, all_suggestions)) + "\n\n"

    # Add deduplicated JSON block
    merged += "```json\n"
    merged += json.dumps({"issues": unique_issues}, indent=2)
    merged += "\n```"

    return merged
```

Import asyncio and update the imports at the top of the file.
  </action>
  <verify>
Run: python -c "from app.ai.reviewer import review_document_chunked, deduplicate_issues, parse_chunk_issues; print('Functions exist')"
Expected: Functions exist
  </verify>
  <done>
review_document_chunked generator function handles parallel chunk processing with Semaphore.
Emits chunk_progress events as chunks complete.
Merges and deduplicates issues after all chunks complete.
Handles partial failures gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update review_document to auto-detect and delegate to chunked review</name>
  <files>backend/app/ai/reviewer.py</files>
  <action>
Modify the existing review_document function to check if chunking is needed:

```python
async def review_document(
    pdf_bytes: bytes,
    extraction: ExtractionResult,
    document_type: str,
    confidence: float,
    output_format: str = "digital",
) -> AsyncGenerator[str, None]:
    """
    Stream AI document review.

    Automatically detects large documents and delegates to chunked review.

    For documents over 40 pages, splits into chunks and processes in parallel.
    For smaller documents, processes directly.

    Yields:
        Text chunks from AI response (or JSON events for chunked review)
    """
    chunker = DocumentChunker()

    if chunker.needs_chunking(extraction.metadata.page_count):
        logger.info(f"Large document ({extraction.metadata.page_count} pages) - using chunked review")
        async for event in review_document_chunked(
            pdf_bytes=pdf_bytes,
            extraction=extraction,
            document_type=document_type,
            confidence=confidence,
            output_format=output_format,
        ):
            yield event
        return

    # Original non-chunked review logic (keep existing code)
    # ... (rest of existing function unchanged)
```

Add the DocumentChunker import at the top:
```python
from .chunker import DocumentChunker, extract_page_range, filter_extraction_for_chunk
```

Keep all existing code for non-chunked review unchanged.
  </action>
  <verify>
Run: python -c "from app.ai.reviewer import review_document; import asyncio; print('review_document exists')"
Expected: review_document exists
  </verify>
  <done>
review_document automatically delegates to chunked review for documents over 40 pages.
Non-chunked review for small documents unchanged.
  </done>
</task>

</tasks>

<verification>
1. Import reviewer module without errors: `python -c "from app.ai.reviewer import *"`
2. deduplicate_issues removes duplicates correctly
3. parse_chunk_issues extracts issues from JSON block
4. review_document_chunked generator exists
5. review_document checks page count and delegates appropriately
</verification>

<success_criteria>
- deduplicate_issues removes duplicate issues based on title + min_page + category
- review_document_chunked processes chunks in parallel with Semaphore(2)
- Chunk progress events emitted as chunks complete
- Merged review includes all sections from all chunks
- Issues deduplicated before final output
- Partial failures handled gracefully with error events
- review_document auto-delegates to chunked review for >40 page documents
</success_criteria>

<output>
After completion, create `.planning/phases/09-chunked-document-review/09-03-SUMMARY.md`
</output>
