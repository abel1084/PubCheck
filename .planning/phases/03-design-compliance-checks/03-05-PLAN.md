---
phase: 03-design-compliance-checks
plan: 05
type: execute
wave: 4
depends_on: ["03-04"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "User can click Check and see compliance results within 30 seconds"
    - "User can see issues grouped by category (Cover, Margins, Typography, Images, Required Elements)"
    - "User can see each issue marked as Error or Warning based on configured severity"
    - "User can see expected vs actual values for each failing check"
    - "User can see which page each issue occurs on"
  artifacts: []
  key_links: []
---

<objective>
User verification checkpoint for Phase 3. Verify that compliance checking works end-to-end: upload PDF, click Check, see categorized results with severity levels and expected vs actual values.

Purpose: Confirms the core value proposition works before proceeding to Phase 4 (AI Verification).
Output: User approval or feedback for iteration
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/03-design-compliance-checks/03-CONTEXT.md
</context>

<tasks>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete compliance checking system:
- Backend check executor with 6 handler types (position, range, font, regex, presence, color)
- Tolerance utilities following CONTEXT.md decisions (margins: minimum only, DPI: 2.5% tolerance, fonts: 0.5pt tolerance)
- POST /api/check/{document_type} endpoint
- Frontend Check Results UI with collapsible categories, severity badges, expected vs actual display
- Check button in header that triggers compliance check
  </what-built>
  <how-to-verify>
1. **Start the application:**
   ```bash
   # Terminal 1: Backend
   cd backend && python -m uvicorn app.main:app --reload --port 8002

   # Terminal 2: Frontend
   cd frontend && npm run dev
   ```

2. **Open browser:** http://localhost:5173

3. **Upload a test PDF:**
   - Drag and drop or click to upload a UNEP-style document
   - Verify document type is detected

4. **Run compliance check:**
   - Click the "Check" button in the header
   - Verify loading state appears briefly
   - Verify results appear in "Check Results" tab

5. **Verify results presentation:**
   - [ ] Categories shown in order: Cover, Margins, Typography, Images, Required Elements
   - [ ] Categories with errors are expanded by default
   - [ ] Categories without errors are collapsed with checkmark
   - [ ] Each issue shows:
     - Severity icon (red for error, yellow for warning)
     - Message describing the issue
     - Expected vs Actual values
     - Page number(s)
   - [ ] Clicking category header collapses/expands it

6. **Verify status badge:**
   - [ ] Pass (green) if no errors
   - [ ] Fail (red) if any errors
   - [ ] Warning (yellow) if only warnings

7. **Test with document that should pass:**
   - Upload a well-designed document
   - [ ] Verify "All checks passed" success banner appears

8. **Test Re-check button:**
   - Modify settings (e.g., change a rule severity)
   - Click Re-check
   - [ ] Verify results update

9. **Verify check timing:**
   - [ ] Results appear within 30 seconds for typical documents
  </how-to-verify>
  <resume-signal>
Type "approved" if all checks pass, or describe issues found:
- What didn't work as expected
- Screenshots or error messages if applicable
  </resume-signal>
</task>

</tasks>

<verification>
User confirms:
1. Check button works and shows loading state
2. Results appear grouped by category
3. Issues show severity, message, expected vs actual, pages
4. Categories collapse/expand correctly
5. Status badge reflects overall result
6. Performance acceptable (< 30 seconds)
</verification>

<success_criteria>
Phase 3 Success Criteria (from ROADMAP.md):
1. User can click "Check" and see compliance results within 30 seconds for typical documents
2. User can see issues grouped by category (Cover, Margins, Typography, Images, Required Elements)
3. User can see each issue marked as Error or Warning based on configured severity
4. User can see expected vs actual values for each failing check
5. User can see which page each issue occurs on
</success_criteria>

<output>
After completion, create `.planning/phases/03-design-compliance-checks/03-05-SUMMARY.md`
</output>
