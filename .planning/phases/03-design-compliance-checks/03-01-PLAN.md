---
phase: 03-design-compliance-checks
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/checks/__init__.py
  - backend/app/checks/models.py
  - backend/app/checks/tolerance.py
  - backend/app/checks/executor.py
autonomous: true

must_haves:
  truths:
    - "CheckIssue model captures rule violations with severity, expected, actual, pages"
    - "Tolerance utilities correctly compare values per CONTEXT.md decisions"
    - "Executor registers 6 check types and delegates to handlers"
  artifacts:
    - path: "backend/app/checks/models.py"
      provides: "CheckIssue, CategoryResult, CheckResult Pydantic models"
      exports: ["CheckIssue", "CategoryResult", "CheckResult"]
    - path: "backend/app/checks/tolerance.py"
      provides: "Tolerance calculation utilities"
      exports: ["points_to_mm", "mm_to_points", "check_margin_minimum", "check_font_size_range", "check_dpi_minimum", "check_logo_size", "normalize_font_name", "normalize_text_for_matching", "check_color_match"]
    - path: "backend/app/checks/executor.py"
      provides: "CheckExecutor with handler registry"
      exports: ["CheckExecutor", "create_executor"]
  key_links:
    - from: "backend/app/checks/executor.py"
      to: "backend/app/checks/models.py"
      via: "imports CheckIssue for return types"
      pattern: "from .models import CheckIssue"
    - from: "backend/app/checks/executor.py"
      to: "backend/app/config/models.py"
      via: "imports Rule, RuleExpected for type annotations"
      pattern: "from app.config.models import Rule"
---

<objective>
Create backend foundation for compliance checking: Pydantic models for check results, centralized tolerance utilities following CONTEXT.md decisions, and executor pattern with handler registry for 6 check types.

Purpose: Establishes the core infrastructure that check handlers (Plan 02) and the API endpoint (Plan 03) will build on.
Output: backend/app/checks/ module with models.py, tolerance.py, executor.py
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/LEARNINGS.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/03-design-compliance-checks/03-CONTEXT.md
@.planning/phases/03-design-compliance-checks/03-RESEARCH.md

@backend/app/models/extraction.py
@backend/app/config/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create check result Pydantic models</name>
  <files>backend/app/checks/__init__.py, backend/app/checks/models.py</files>
  <action>
Create backend/app/checks/ module with Pydantic v1 models:

1. Create `backend/app/checks/__init__.py` exporting models and executor

2. Create `backend/app/checks/models.py` with:

```python
class CheckIssue(BaseModel):
    """A single compliance issue found during checking."""
    rule_id: str
    rule_name: str
    severity: Literal["error", "warning"]
    message: str  # Human-readable description
    expected: Optional[str]  # Formatted for display
    actual: Optional[str]  # Formatted for display
    pages: List[int]  # 1-indexed page numbers
    how_to_fix: Optional[str] = None  # Hint for obvious fixes

class CategoryResult(BaseModel):
    """Results for a single category of checks."""
    category_id: str
    category_name: str
    issues: List[CheckIssue]
    error_count: int
    warning_count: int

class CheckResult(BaseModel):
    """Complete compliance check result."""
    document_type: str
    categories: List[CategoryResult]
    total_errors: int
    total_warnings: int
    status: Literal["pass", "fail", "warning"]  # pass=no errors, fail=has errors, warning=warnings only
    check_duration_ms: int
```

Use Pydantic v1 patterns (class Config, List/Optional from typing) per Phase 1 decision.
  </action>
  <verify>python -c "from app.checks.models import CheckIssue, CategoryResult, CheckResult; print('Models imported OK')"</verify>
  <done>CheckIssue, CategoryResult, CheckResult models importable with all fields typed</done>
</task>

<task type="auto">
  <name>Task 2: Create tolerance utilities module</name>
  <files>backend/app/checks/tolerance.py</files>
  <action>
Create `backend/app/checks/tolerance.py` with centralized tolerance calculations per CONTEXT.md:

1. Unit conversion:
   - `points_to_mm(points: float) -> float`: Convert points to mm (1 pt = 25.4/72 mm)
   - `mm_to_points(mm: float) -> float`: Convert mm to points

2. Margin checking (CONTEXT.md: "Check minimum only - flag when content too close to edge"):
   - `check_margin_minimum(actual_mm: float, min_mm: float) -> bool`: Returns True if actual >= min

3. Font size checking (CONTEXT.md: "+/-0.5pt tolerance"):
   - `check_font_size_range(actual_pt: float, min_pt: float, max_pt: float, tolerance: float = 0.5) -> bool`

4. DPI checking (CONTEXT.md: "2.5% tolerance below minimum"):
   - `check_dpi_minimum(actual_dpi: float, min_dpi: float, tolerance_pct: float = 2.5) -> bool`

5. Logo size checking (CONTEXT.md: "+/-1mm tolerance"):
   - `check_logo_size(actual_mm: float, min_mm: float, tolerance: float = 1.0) -> bool`

6. Font name normalization (reuse logic from text_processor.py):
   - `normalize_font_name(font_name: str) -> str`: Strip 6-char subset prefix if present

7. Text normalization (CONTEXT.md: "Case-insensitive, whitespace normalized"):
   - `normalize_text_for_matching(text: str) -> str`: Unicode normalize, collapse whitespace, casefold

8. Color matching (CONTEXT.md: "Allow near-matches, small delta per channel"):
   - `check_color_match(actual_rgb: int, expected_hex: str, tolerance: int = 5) -> bool`: Per-channel RGB delta

Import unicodedata and re from stdlib. NO external dependencies.
  </action>
  <verify>python -c "from app.checks.tolerance import points_to_mm, check_dpi_minimum, normalize_font_name; print(f'72pt = {points_to_mm(72):.2f}mm'); print(f'293 DPI >= 300 min: {check_dpi_minimum(293, 300)}'); print(f'ABCDEF+Roboto -> {normalize_font_name(\"ABCDEF+Roboto\")}')"</verify>
  <done>All tolerance functions work correctly: 72pt = 25.40mm, 293 DPI passes 300 min check, font names normalized</done>
</task>

<task type="auto">
  <name>Task 3: Create check executor with handler registry</name>
  <files>backend/app/checks/executor.py</files>
  <action>
Create `backend/app/checks/executor.py` with handler registry pattern:

```python
from typing import Callable, Dict, List
from app.models.extraction import ExtractionResult
from app.config.models import Rule, RuleExpected
from .models import CheckIssue

# Handler signature: (extraction, rule, expected) -> list[CheckIssue]
CheckHandler = Callable[[ExtractionResult, Rule, RuleExpected], List[CheckIssue]]

class CheckExecutor:
    """Execute compliance checks against extracted PDF data."""

    def __init__(self):
        self._handlers: Dict[str, CheckHandler] = {}

    def register(self, check_type: str, handler: CheckHandler) -> None:
        """Register a handler for a check type."""
        self._handlers[check_type] = handler

    def execute_rule(self, extraction: ExtractionResult, rule: Rule) -> List[CheckIssue]:
        """Execute a single rule against extraction data."""
        # Skip disabled rules silently per CONTEXT.md
        if not rule.enabled:
            return []

        handler = self._handlers.get(rule.check_type)
        if not handler:
            # Unknown check type - log warning, return empty (don't crash)
            return []

        try:
            return handler(extraction, rule, rule.expected)
        except Exception as e:
            # Return error issue for failed check per CONTEXT.md
            return [CheckIssue(
                rule_id=rule.name,
                rule_name=rule.name,
                severity="error",
                message=f"Check failed unexpectedly: {str(e)}",
                expected=None,
                actual=None,
                pages=[],
            )]

def create_executor() -> CheckExecutor:
    """Create executor with all handlers registered."""
    executor = CheckExecutor()
    # Handlers will be registered in Plan 02
    # Placeholder registrations - will be replaced
    return executor
```

The create_executor() function will be updated in Plan 02 to import and register actual handlers.
  </action>
  <verify>python -c "from app.checks.executor import CheckExecutor, create_executor; e = create_executor(); print('Executor created OK')"</verify>
  <done>CheckExecutor class with register() and execute_rule() methods, create_executor() factory function</done>
</task>

</tasks>

<verification>
Run from backend directory:
```bash
cd backend
python -c "
from app.checks.models import CheckIssue, CategoryResult, CheckResult
from app.checks.tolerance import points_to_mm, check_dpi_minimum, normalize_font_name, check_color_match
from app.checks.executor import CheckExecutor, create_executor

# Test models
issue = CheckIssue(rule_id='test', rule_name='Test', severity='error', message='Test', expected='1', actual='2', pages=[1])
print(f'CheckIssue: {issue.rule_id}, severity={issue.severity}')

# Test tolerance
print(f'72pt = {points_to_mm(72):.2f}mm')
print(f'293 DPI >= 300: {check_dpi_minimum(293, 300)}')
print(f'Color match: {check_color_match(0x00AEEF, \"#00AEEF\")}')

# Test executor
executor = create_executor()
print('Executor handlers:', len(executor._handlers))

print('All checks module tests passed!')
"
```
</verification>

<success_criteria>
1. backend/app/checks/ module exists with __init__.py, models.py, tolerance.py, executor.py
2. All Pydantic models use v1 patterns (class Config, typing imports)
3. Tolerance functions follow CONTEXT.md decisions exactly
4. CheckExecutor handles disabled rules and exceptions gracefully
5. All modules importable without errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-design-compliance-checks/03-01-SUMMARY.md`
</output>
