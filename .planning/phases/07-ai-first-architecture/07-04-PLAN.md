---
phase: 07-ai-first-architecture
plan: 04
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - backend/app/ai/reviewer.py
  - backend/app/ai/prompts.py
  - backend/app/ai/schemas.py
autonomous: true

must_haves:
  truths:
    - "Reviewer loads rules context based on document type"
    - "System prompt produces collegial review style"
    - "Response follows Overview/Needs Attention/Looking Good/Suggestions structure"
    - "Measurements cited in issue descriptions"
  artifacts:
    - path: "backend/app/ai/reviewer.py"
      provides: "Document review orchestration"
      min_lines: 50
      exports: ["review_document"]
    - path: "backend/app/ai/prompts.py"
      provides: "System and user prompt builders"
      min_lines: 60
      contains: "collegial"
    - path: "backend/app/ai/schemas.py"
      provides: "Review request/response schemas"
      min_lines: 30
  key_links:
    - from: "backend/app/ai/reviewer.py"
      to: "backend/app/ai/client.py"
      via: "review_document_stream call"
      pattern: "review_document_stream"
    - from: "backend/app/ai/reviewer.py"
      to: "backend/app/config/rules_context/"
      via: "load_rules_context"
      pattern: "rules_context"
---

<objective>
Create the AI reviewer module with prompts for collegial review style.

Purpose: Implement the core review logic (ARCH-06) that sends PDF + extraction + rules to Claude and produces helpful colleague-style feedback organized by priority sections.

Output: reviewer.py with document review function, prompts.py with collegial system/user prompts, schemas.py with review types.
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/07-ai-first-architecture/07-CONTEXT.md
@.planning/phases/07-ai-first-architecture/07-RESEARCH.md
@.planning/phases/07-ai-first-architecture/07-02-SUMMARY.md
@backend/app/ai/client.py
@backend/app/config/rules_context/factsheet.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update AI schemas for review</name>
  <files>backend/app/ai/schemas.py</files>
  <action>
Update `backend/app/ai/schemas.py` to remove old page-by-page analysis schemas and add simple review schemas:

```python
"""
Schemas for AI-powered document review.
Simplified for single-call document review with streaming response.
"""
from typing import Optional
from pydantic import BaseModel, Field


class ReviewRequest(BaseModel):
    """Request for AI document review."""
    document_type: str = Field(..., description="Type of document being reviewed")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Document type detection confidence")


class ReviewMetadata(BaseModel):
    """Metadata about the review process."""
    document_type: str
    page_count: int
    model_used: str
    estimated_tokens: Optional[int] = None
```

Keep the file minimal - the actual review content is streamed as markdown text, not structured JSON.
  </action>
  <verify>
- File contains ReviewRequest and ReviewMetadata classes
- Old DocumentAnalysisResult and page-by-page schemas removed
- `python -c "from app.ai.schemas import ReviewRequest, ReviewMetadata; print('OK')"`
  </verify>
  <done>
- AI schemas simplified for streaming review
- Old page-by-page schemas removed
  </done>
</task>

<task type="auto">
  <name>Task 2: Create collegial review prompts</name>
  <files>backend/app/ai/prompts.py</files>
  <action>
Rewrite `backend/app/ai/prompts.py` with collegial review system and user prompts:

```python
"""
Prompts for AI-powered document review.
Produces collegial, helpful feedback organized by priority sections.
"""

SYSTEM_PROMPT_TEMPLATE = '''You are a helpful colleague reviewing UNEP publications for design compliance. Your review should read like feedback from an experienced designer who wants the document to succeed.

## Review Style
- Be collegial and constructive: "The logo looks a bit small at 18mm - spec asks for 20mm minimum"
- Cite measurements when relevant: sizes, margins, DPI values
- Use honest hedging when uncertain: "This might be intentional, but..."
- No formal confidence scores - express uncertainty naturally in prose

## Response Structure
Organize your review into these sections, using markdown headers:

### Overview
Brief 2-3 sentence summary of the document's compliance state. Mention the document type and overall impression.

### Needs Attention
Issues that should be fixed before publication. Group related issues naturally.
Each issue should explain what's wrong and suggest a fix.
If there are no issues, write "Everything looks good!" and skip to Looking Good.

### Looking Good
Specific things the document does well. Acknowledge good design work.
Mention 2-3 positive observations.

### Suggestions
Minor improvements that would enhance the document but aren't requirements.
Optional enhancements or stylistic suggestions.

## Common Pitfalls to Avoid in Your Analysis
- Full-bleed images are intentional design, not margin violations
- Decorative elements (thin lines, bullets, icons under ~5mm) don't need DPI checks
- Small accent images may intentionally use lower resolution
- Headers/footers have different margin rules than body content
- Font variations within a family (Roboto vs Roboto Condensed) are acceptable

## What You're Checking
You'll receive:
1. The original PDF document (you can see all pages)
2. Extracted measurements as JSON (fonts, images, margins, text blocks)
3. Document type with confidence score - validate this matches the actual document

{rules_context}
'''


def build_system_prompt(rules_context: str) -> str:
    """
    Build system prompt with rules context injected.

    Args:
        rules_context: Markdown rules for the document type

    Returns:
        Complete system prompt
    """
    return SYSTEM_PROMPT_TEMPLATE.format(rules_context=rules_context)


def build_user_prompt(
    extraction_json: str,
    document_type: str,
    confidence: float,
) -> str:
    """
    Build user prompt with extraction data and document type.

    Args:
        extraction_json: JSON string of ExtractionResult
        document_type: Detected document type
        confidence: Detection confidence (0-1)

    Returns:
        User prompt for document review
    """
    confidence_note = ""
    if confidence < 0.8:
        confidence_note = f"\\n(Please verify this is actually a {document_type} - detection confidence is only {confidence:.0%})"

    return f'''Please review this {document_type} for design compliance.

Document type confidence: {confidence:.0%}{confidence_note}

## Extracted Measurements
The following JSON contains measurements extracted from the PDF. Use this data to verify specific values (font sizes, margins, DPI, etc.) but also visually inspect the document for issues the extraction might miss.

```json
{extraction_json}
```

Review the document and provide your assessment using the section structure from your instructions (Overview, Needs Attention, Looking Good, Suggestions).'''
```

**Key features:**
- Collegial tone: "helpful colleague", not auditor
- Section structure: Overview, Needs Attention, Looking Good, Suggestions
- Common pitfalls: Full-bleed images, decorative elements
- Measurement citations: Explicit instruction to cite specific values
- Uncertainty hedging: "might be intentional" style
  </action>
  <verify>
- File contains SYSTEM_PROMPT_TEMPLATE with collegial instructions
- build_system_prompt and build_user_prompt functions exist
- Template includes all 4 section headers
- Common pitfalls section present
  </verify>
  <done>
- Prompts rewritten for collegial review style
- Section structure enforced in template
- Common false positives documented
  </done>
</task>

<task type="auto">
  <name>Task 3: Create document reviewer module</name>
  <files>backend/app/ai/reviewer.py</files>
  <action>
Create `backend/app/ai/reviewer.py` that orchestrates document review:

```python
"""
Document reviewer module.
Orchestrates AI document review by combining PDF, extraction, and rules context.
"""
import json
import logging
from pathlib import Path
from typing import AsyncGenerator

from app.models.extraction import ExtractionResult

from .client import get_ai_client
from .prompts import build_system_prompt, build_user_prompt


logger = logging.getLogger(__name__)

# Rules context directory
RULES_CONTEXT_DIR = Path(__file__).parent.parent / "config" / "rules_context"

# Map document type IDs to rules context files
RULES_CONTEXT_MAP = {
    "factsheet": "factsheet.md",
    "policy-brief": "brief.md",
    "issue-note": "brief.md",  # Uses same rules as brief
    "working-paper": "working_paper.md",
    "publication": "publication.md",
}


def load_rules_context(document_type: str) -> str:
    """
    Load rules context markdown for document type.

    Args:
        document_type: Document type ID (factsheet, publication, etc.)

    Returns:
        Markdown rules context content

    Raises:
        FileNotFoundError: If rules file doesn't exist
    """
    filename = RULES_CONTEXT_MAP.get(document_type)
    if not filename:
        logger.warning(f"Unknown document type: {document_type}, using publication rules")
        filename = "publication.md"

    rules_path = RULES_CONTEXT_DIR / filename
    if not rules_path.exists():
        raise FileNotFoundError(f"Rules context not found: {rules_path}")

    return rules_path.read_text(encoding="utf-8")


async def review_document(
    pdf_bytes: bytes,
    extraction: ExtractionResult,
    document_type: str,
    confidence: float,
) -> AsyncGenerator[str, None]:
    """
    Stream AI document review.

    Sends PDF + extraction + rules context to Claude and yields text chunks.

    Args:
        pdf_bytes: Raw PDF file bytes
        extraction: Extracted document data
        document_type: Document type ID
        confidence: Document type detection confidence

    Yields:
        Text chunks from AI response

    Raises:
        AIClientError: On API errors
        FileNotFoundError: If rules context missing
    """
    # Load rules context
    rules_context = load_rules_context(document_type)
    logger.info(f"Loaded rules context for {document_type}: {len(rules_context)} chars")

    # Build prompts
    system_prompt = build_system_prompt(rules_context)
    extraction_json = json.dumps(extraction.dict(), indent=2, default=str)
    user_prompt = build_user_prompt(extraction_json, document_type, confidence)

    logger.info(f"Starting review: {extraction.metadata.page_count} pages, type={document_type}")

    # Stream from AI client
    client = get_ai_client()
    async for chunk in client.review_document_stream(
        pdf_bytes=pdf_bytes,
        user_prompt=user_prompt,
        system_prompt=system_prompt,
    ):
        yield chunk

    logger.info("Review stream complete")
```

**Key features:**
- Rules context loading: Maps document types to markdown files
- Prompt building: Combines rules, extraction JSON, document type
- Streaming pass-through: Yields chunks from AI client
- Error handling: FileNotFoundError for missing rules
  </action>
  <verify>
- File exists with load_rules_context and review_document functions
- RULES_CONTEXT_MAP covers all document types
- `python -c "from app.ai.reviewer import review_document, load_rules_context; print('OK')"`
  </verify>
  <done>
- Document reviewer module created
- Rules context loading implemented
- Streaming review function ready
  </done>
</task>

</tasks>

<verification>
1. All three files exist and import successfully
2. `python -c "from app.ai.reviewer import review_document; from app.ai.prompts import build_system_prompt; from app.ai.schemas import ReviewRequest; print('OK')"`
3. Prompts contain collegial instructions and section structure
4. Reviewer loads rules context from correct directory
</verification>

<success_criteria>
- AI schemas simplified for streaming review
- Prompts produce collegial, sectioned output
- Reviewer orchestrates PDF + extraction + rules context
- All modules import successfully
</success_criteria>

<output>
After completion, create `.planning/phases/07-ai-first-architecture/07-04-SUMMARY.md`
</output>
