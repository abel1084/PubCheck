---
phase: 04-ai-verification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/ai/__init__.py
  - backend/app/ai/client.py
  - backend/app/ai/schemas.py
  - backend/app/ai/prompts.py
  - backend/app/ai/renderer.py
autonomous: true

must_haves:
  truths:
    - "AI client can call Claude API with images"
    - "Page images render at screen resolution (72 DPI)"
    - "Checklist generates from YAML rules at runtime"
    - "Structured outputs parse into Pydantic models"
  artifacts:
    - path: "backend/app/ai/client.py"
      provides: "Anthropic client wrapper with retry logic"
      exports: ["AIClient", "get_ai_client"]
    - path: "backend/app/ai/schemas.py"
      provides: "Pydantic response models"
      exports: ["AIFinding", "PageAnalysisResult", "DocumentAnalysisResult"]
    - path: "backend/app/ai/prompts.py"
      provides: "Prompt templates and checklist generation"
      exports: ["build_analysis_prompt", "generate_checklist"]
    - path: "backend/app/ai/renderer.py"
      provides: "PDF page to base64 image rendering"
      exports: ["render_page_to_base64"]
  key_links:
    - from: "backend/app/ai/client.py"
      to: "anthropic SDK"
      via: "messages.create with vision"
      pattern: "client\\.messages\\.create"
    - from: "backend/app/ai/prompts.py"
      to: "YAML rule templates"
      via: "generate_checklist reads rules"
      pattern: "RuleService|get_merged_rules"
---

<objective>
Create AI infrastructure module with Anthropic client, Pydantic schemas, prompt templates, and page renderer.

Purpose: Provides the foundation for AI-powered document analysis - Claude API integration, structured response handling, and PDF-to-image conversion.

Output: backend/app/ai/ module with client, schemas, prompts, and renderer ready for integration.
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/LEARNINGS.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-verification/04-CONTEXT.md
@.planning/phases/04-ai-verification/04-RESEARCH.md
@backend/app/checks/models.py
@backend/app/config/service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AI client with retry logic</name>
  <files>backend/app/ai/__init__.py, backend/app/ai/client.py</files>
  <action>
Create `backend/app/ai/` module with Anthropic client wrapper.

**client.py:**
- `AIClient` class wrapping anthropic.Anthropic
- Get API key from `ANTHROPIC_API_KEY` environment variable
- Use model from `AI_MODEL` env var, default to `claude-sonnet-4-20250514`
- `analyze_page(image_data: str, prompt: str) -> dict` method:
  - Calls messages.create with image content block (base64, image/png)
  - Uses temperature=0 for deterministic results
  - max_tokens=4096 for detailed analysis
  - Returns parsed JSON from response text
- Retry logic: 2-3 retries on API errors with exponential backoff (1s, 2s, 4s)
- 30-second timeout per request per CONTEXT.md
- Singleton pattern via `get_ai_client()` function

**__init__.py:**
- Export AIClient, get_ai_client

Use Pydantic v1 patterns (class Config, not model_config).
Handle missing API key gracefully (raise descriptive error on first use, not import).
  </action>
  <verify>
```bash
cd backend && python -c "
from app.ai import AIClient, get_ai_client
client = get_ai_client()
print(f'Client created: {client is not None}')
print(f'Model: {client._model}')
"
```
  </verify>
  <done>AIClient singleton exists with retry logic and 30-second timeout</done>
</task>

<task type="auto">
  <name>Task 2: Create Pydantic schemas for AI responses</name>
  <files>backend/app/ai/schemas.py</files>
  <action>
Create Pydantic v1 models for AI analysis responses.

**schemas.py:**
```python
from typing import List, Literal, Optional
from pydantic import BaseModel, Field

class AIFinding(BaseModel):
    """Single finding from AI analysis."""
    check_name: str = Field(description="Name of the check (e.g., 'UNEP Logo Position')")
    passed: bool = Field(description="Whether the check passed")
    confidence: Literal["high", "medium", "low"] = Field(description="Confidence level")
    message: str = Field(description="Brief description of finding")
    reasoning: Optional[str] = Field(None, description="Explanation for non-obvious findings")
    location: Optional[str] = Field(None, description="Descriptive location (e.g., 'top-right corner')")
    suggestion: Optional[str] = Field(None, description="Fix suggestion for failures")

    class Config:
        extra = "forbid"

class PageAnalysisResult(BaseModel):
    """Analysis result for a single page."""
    page_number: int = Field(description="1-indexed page number")
    findings: List[AIFinding] = Field(default_factory=list)
    error: Optional[str] = Field(None, description="Error message if analysis failed")

    class Config:
        extra = "forbid"

class DocumentAnalysisResult(BaseModel):
    """Complete document analysis result."""
    page_results: List[PageAnalysisResult] = Field(default_factory=list)
    document_summary: Optional[str] = Field(None, description="Cross-page consistency notes")
    total_findings: int = Field(default=0)
    analysis_duration_ms: int = Field(default=0)

    class Config:
        extra = "forbid"
```

Add exports to __init__.py: AIFinding, PageAnalysisResult, DocumentAnalysisResult
  </action>
  <verify>
```bash
cd backend && python -c "
from app.ai.schemas import AIFinding, PageAnalysisResult, DocumentAnalysisResult
finding = AIFinding(check_name='test', passed=True, confidence='high', message='OK')
print(f'Finding: {finding.dict()}')
result = PageAnalysisResult(page_number=1, findings=[finding])
print(f'Page result: {result.dict()}')
"
```
  </verify>
  <done>AIFinding, PageAnalysisResult, DocumentAnalysisResult models validate correctly</done>
</task>

<task type="auto">
  <name>Task 3: Create prompts module with checklist generation</name>
  <files>backend/app/ai/prompts.py</files>
  <action>
Create prompt templates and checklist generator.

**prompts.py:**
- `generate_checklist(rules: Dict[str, CategoryConfig]) -> str` function:
  - Takes merged rules from RuleService
  - Generates simplified checklist of enabled rules only
  - Groups by category (Cover, Margins, Typography, Images, Required Elements)
  - Format: bullet points with check name and requirement
  - Example output:
    ```
    ## Cover
    - UNEP Logo: Must be present in top-right corner, minimum 20mm
    - Title: Roboto Flex font, 28-34pt

    ## Margins
    - Top margin: minimum 20mm
    - Bottom margin: minimum 20mm
    ```

- `build_analysis_prompt(checklist: str, extraction_summary: str) -> str` function:
  - Combines checklist with page-specific context
  - Includes instruction to return JSON array of findings
  - Specifies confidence levels (high/medium/low)
  - Instructs to include reasoning only for non-obvious findings
  - Instructs to include suggestions in reasoning section

- `SYSTEM_PROMPT` constant:
  - Role: document compliance analyst
  - Task: analyze page image against checklist
  - Output format: JSON with findings array
  - Confidence calibration guidelines

Add exports to __init__.py: generate_checklist, build_analysis_prompt, SYSTEM_PROMPT
  </action>
  <verify>
```bash
cd backend && python -c "
from app.ai.prompts import generate_checklist, build_analysis_prompt, SYSTEM_PROMPT
print(f'System prompt length: {len(SYSTEM_PROMPT)} chars')
print(f'Has JSON instruction: {\"JSON\" in SYSTEM_PROMPT}')
# Test with mock rules
mock_checklist = '## Cover\n- Logo: top-right'
prompt = build_analysis_prompt(mock_checklist, 'Page 1 of factsheet')
print(f'Prompt includes checklist: {\"Logo\" in prompt}')
"
```
  </verify>
  <done>generate_checklist creates rule summary, build_analysis_prompt combines for API call</done>
</task>

<task type="auto">
  <name>Task 4: Create page renderer at 72 DPI</name>
  <files>backend/app/ai/renderer.py</files>
  <action>
Create PDF page to base64 image renderer using PyMuPDF.

**renderer.py:**
- `render_page_to_base64(pdf_path: str, page_num: int, dpi: int = 72) -> str` function:
  - Opens PDF with pymupdf.open()
  - Gets page by 0-indexed page_num
  - Calculates zoom factor: dpi / 72.0
  - Renders with page.get_pixmap(matrix=Matrix(zoom, zoom), alpha=False)
  - Converts to PNG bytes with pix.tobytes("png")
  - Returns base64.standard_b64encode().decode("utf-8")
  - Closes document in finally block

- Use 72 DPI (screen resolution) per CONTEXT.md decision
- No max dimension scaling needed at 72 DPI (typical A4 = 595x842 pixels)

Add exports to __init__.py: render_page_to_base64
  </action>
  <verify>
```bash
cd backend && python -c "
from app.ai.renderer import render_page_to_base64
import os
# Check function exists and has correct signature
import inspect
sig = inspect.signature(render_page_to_base64)
params = list(sig.parameters.keys())
print(f'Parameters: {params}')
print(f'Has pdf_path: {\"pdf_path\" in params}')
print(f'Has page_num: {\"page_num\" in params}')
print(f'Has dpi: {\"dpi\" in params}')
"
```
  </verify>
  <done>render_page_to_base64 converts PDF page to base64 PNG at 72 DPI</done>
</task>

</tasks>

<verification>
All AI infrastructure components exist and can be imported:
```bash
cd backend && python -c "
from app.ai import (
    AIClient, get_ai_client,
    AIFinding, PageAnalysisResult, DocumentAnalysisResult,
    generate_checklist, build_analysis_prompt, SYSTEM_PROMPT,
    render_page_to_base64
)
print('All AI infrastructure imports successful')
"
```
</verification>

<success_criteria>
1. AIClient wraps Anthropic SDK with retry logic and 30-second timeout
2. Pydantic schemas validate AI responses with confidence levels
3. Checklist generator creates bullet-point summary from YAML rules
4. Page renderer converts PDF pages to base64 PNG at 72 DPI
5. All components export from backend/app/ai/ module
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-verification/04-01-SUMMARY.md`
</output>
