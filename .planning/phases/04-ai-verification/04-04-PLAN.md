---
phase: 04-ai-verification
plan: 04
type: execute
wave: 4
depends_on: ["04-03"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "User can upload PDF and trigger AI analysis"
    - "User sees progress indicator during analysis"
    - "User sees AI findings with confidence indicators"
    - "AI analysis completes within 60 seconds for typical documents"
  artifacts: []
  key_links: []
---

<objective>
Verify AI analysis works end-to-end with real PDFs.

Purpose: Ensure all AI verification components work together - user can analyze a document and see meaningful AI findings.

Output: Phase 4 verified and approved by user.
</objective>

<execution_context>
@C:\Users\abelb\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\abelb\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-verification/04-CONTEXT.md
@.planning/phases/04-ai-verification/04-03-SUMMARY.md
</context>

<tasks>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
AI verification system with:
1. Backend AI infrastructure (client, schemas, prompts, renderer)
2. Document analyzer with concurrent page processing
3. REST API endpoint for AI analysis
4. Frontend integration with progress indicator and confidence display
  </what-built>
  <how-to-verify>

**Prerequisites:**
- Set `ANTHROPIC_API_KEY` environment variable with your Claude API key
- Have a test PDF ready (ideally a UNEP publication)

**Test Steps:**

1. **Start the application:**
   ```
   python start.py
   ```

2. **Upload a PDF:**
   - Drag a test PDF onto the upload area
   - Wait for extraction to complete
   - Verify extraction data displays in tabs

3. **Run AI Analysis:**
   - Click "Analyze with AI" button (should be green outline, next to Check)
   - Observe progress indicator appears ("Analyzing X pages...")
   - Wait for analysis to complete (should be under 60 seconds)

4. **Verify AI Results:**
   - Scroll to "AI Analysis" section in Check Results
   - Check that findings appear with:
     - Check name and pass/fail status
     - Confidence indicator (icon + tooltip) for medium/low
     - Expandable reasoning section
     - Low-confidence findings have muted styling

5. **Test Error Handling:**
   - (Optional) Test with invalid API key to see error message
   - (Optional) Test with large PDF to verify timeout handling

**Expected Behavior:**
- Progress indicator shows during analysis
- AI findings appear in dedicated section
- Confidence indicators visible for non-high confidence
- Analysis completes within 60 seconds for typical 10-page document
- Errors display clearly if API key missing or analysis fails

  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 4, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
User confirms:
- AI analysis button visible and functional
- Progress indicator shows during analysis
- AI findings display with confidence indicators
- Reasoning expandable when present
- Performance acceptable (under 60 seconds)
</verification>

<success_criteria>
1. User can trigger AI analysis from UI
2. Progress indicator provides feedback during analysis
3. AI findings display with proper formatting
4. Confidence levels visible for medium/low
5. Analysis completes in reasonable time
6. Errors handled gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-verification/04-04-SUMMARY.md`
</output>
